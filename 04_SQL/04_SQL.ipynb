{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dkASU227TuUJ"
   },
   "source": [
    "# 4.SQL and Dataframes\n",
    "\n",
    "References:\n",
    "\n",
    "* Spark-SQL, <https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n",
    "\n",
    "\n",
    "# 4.1  Example Walkthrough\n",
    "Follow the Spark SQL and Dataframes Examples below!\n",
    "\n",
    "### Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38298,
     "status": "ok",
     "timestamp": 1617112910170,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "LbZC0TEZTuUL",
    "outputId": "bcef61e9-dc5a-44a3-eff4-2943686c6b05"
   },
   "outputs": [],
   "source": [
    "!pip install -q pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1617113437577,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "PiPbzzOQcORs",
    "outputId": "190e9f8d-15e4-4ca6-9241-465753021da3"
   },
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6579,
     "status": "ok",
     "timestamp": 1617113444953,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "1hiLvAIOTuUL",
    "outputId": "b291a61b-2653-4758-dcc3-545ff3f96065"
   },
   "outputs": [],
   "source": [
    "# Initialize PySpark\n",
    "import os, sys\n",
    "APP_NAME = \"PySpark Lecture\"\n",
    "SPARK_MASTER=\"local[2]\"\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import Row\n",
    "conf=pyspark.SparkConf()\n",
    "conf=pyspark.SparkConf().setAppName(APP_NAME).set(\"spark.local.dir\", os.path.join(os.getcwd(), \"tmp\"))\n",
    "sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29hDbwJgTuUM"
   },
   "source": [
    "### Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3446,
     "status": "ok",
     "timestamp": 1617113444953,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "sdkuaf_VVUxy",
    "outputId": "5a714bb1-435d-435b-8a42-86ed761d2650"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/scalable-infrastructure/exercise-2024/main/data/example.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1617113464836,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "DaNcpjX0TuUM",
    "outputId": "7ec3dfde-2712-4dd6-bd2d-d278ceeabe18"
   },
   "outputs": [],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKlgdJVdTuUM"
   },
   "source": [
    "### Creating Rows\n",
    "\n",
    "Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1617113556308,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "S3ZDj3uvTuUN"
   },
   "outputs": [],
   "source": [
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def csv_to_row(line):\n",
    "    parts = line.split(\",\")\n",
    "    row = Row(\n",
    "      name=parts[0],\n",
    "      company=parts[1],\n",
    "      title=parts[2]\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "rows = csv_lines.map(csv_to_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRmw3MIBTuUN"
   },
   "source": [
    "### Creating DataFrames from RDDs\n",
    "\n",
    "Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10320,
     "status": "ok",
     "timestamp": 1617113568045,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "R0Xj9BfITuUN",
    "outputId": "1b6ec660-a930-45ea-9d2c-2f865ad707fc"
   },
   "outputs": [],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "rows_df = rows.toDF()\n",
    "\n",
    "# Register the DataFrame for Spark SQL\n",
    "rows_df.registerTempTable(\"executives\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS total\n",
    "  FROM executives\n",
    "  GROUP BY name\n",
    "\"\"\")\n",
    "job_counts.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "job_counts.rdd.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2ofD74jfTuUO"
   },
   "source": [
    "# 4.2-4.9 NASA DataSet\n",
    "\n",
    "4.2 Create a Spark-SQL table with fields for IP/Host and Response Code from the NASA Log file! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uELQAPjYTuUO"
   },
   "source": [
    "4.3 Run an SQL query that outputs the number of occurrences of each HTTP response code!\n",
    "\n",
    "4.4 Implement the same Query using the Dataframe API!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c22wAS25TuUO"
   },
   "source": [
    "4.5 Cachen Sie den Dataframe und f端hren Sie dieselbe Query nochmals aus! Messen Sie die Laufzeit f端r das Cachen und f端r die Ausf端hrungszeit der Query!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DfsKn2IGTuUP"
   },
   "source": [
    "4.6 Performance Analysis - Weak Scaling: \n",
    "* Create RDDs with 2x, 4x, 8x and 16x of the size of the NASA log dataset! Persist the dataset in the Spark Cache! Use an appropriate number of cores (e.g. 8 or 16)!\n",
    "* Measure and plot the response times for all datasets using a constant number of cores!\n",
    "* Plot the results!\n",
    "* Explain the results!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U2j_E8RSTuUP"
   },
   "source": [
    "4.7 Performance Analysis - Weak Scaling: \n",
    "\n",
    "  * **Measure the runtime for the query for 1, 2, 4 worker threads (local[n]) for 1x and 16x datasets!** Datasets cached in Memory! Note that Collab environment only has two cores!\n",
    "  * Compute the speedup and efficiency!\n",
    "  * Plot the responses!\n",
    "  * Explain the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_74seWVb9kW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 05_SQL.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/scalable-infrastructure/exercise-students-2021/blob/master/05_SQL/05_SQL.ipynb",
     "timestamp": 1617113592153
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
